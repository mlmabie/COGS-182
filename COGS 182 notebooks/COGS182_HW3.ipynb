{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COGS182_HW3.ipynb","provenance":[{"file_id":"1oLO2_HDUlJdj9xeVeWKgrLRVEwmqcG2a","timestamp":1612083522062}],"collapsed_sections":[],"authorship_tag":"ABX9TyPcTRwNbl2mxavsBO5eben0"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1pRwde7UCX2A"},"source":["## COGS 182 Homework 3\n","Malachi Mabie \n","30 Jan 2021"]},{"cell_type":"markdown","metadata":{"id":"FfkAfm6wqtSf"},"source":["### 1. (2 points) What are the dangers of rewarding subgoals? Give a new example of a situation in which rewarding a subgoal may lead to undesired behavior.\n","\n","May trap it in short-term loop ie optimizing for accumulation over winning; overly constrains problem;\n","\n","If you reward sub-goals, then you're going to try to optimize those actions rather than actions that directly contribute to the overall goal. This could trap you in a short-term loop kind of thing. There are lots of cases like this. Open-AI published examples. Accumulating points but not winning the game... When you reward subgoals, you're kind of injecting knowledge of how you think the problem can be solved, constricting the solution space. Rewarding state configurations can be very good when referencing expert knowledge (like rewarding when you copy a genius chess player's moves)."]},{"cell_type":"markdown","metadata":{"id":"xl7z7Xpzbz4W"},"source":["### 2. (1 point) Suppose $\\gamma$ = 0.6 and the following sequence of rewards is received R1 = -4, R2 = 2, R3 = 6, R4 = 6, and R5 = 2, with T = 5. What are G0, G1, .. ., G5? Hints: Work backwards. Equation 3.9 shows the indexing."]},{"cell_type":"markdown","metadata":{"id":"j636RgHBcBf4"},"source":["As $\\gamma$ approaches 1, the return objective takes future rewards into account more strongly; the agent becomes more farsighted. Equation 3.9 shows how the returns at successive time steps are related to eachother:\n","$$\n","\\begin{align}\n","G_t &= R_{t+1} + \\gamma R_{t+2}+\\gamma^2R_{t+3}+\\gamma^3R_{t+4}+\\dots\\\\\n","&= R_{t+1} + \\gamma(R_{t+2}+\\gamma R_{t+3}+\\gamma^2R_{t+4}+\\dots)\\\\\n","&= R_{t+1} + \\gamma G_{t+1}\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"nYwCn58lesDo"},"source":["$$\n","\\begin{align}\n","G_1 &= R_2 + \\gamma(R_3 +\\gamma R_4 + \\gamma^2R_5)\\\\\n"," &= 2 + 0.6(6 + 0.6\\times 6 + 0.6^2\\times 2) = 8.192\\\\\n","G_0 &= R_1 + \\gamma G_1\\\\\n"," &= -4 + 0.6 \\times 8.192 = 0.915\\\\\n"," G_3 &= R_4 + \\gamma R_5 = 6.3\\\\\n"," G_2 &= R_3 + \\gamma G_3 = 6 + .6\\times 6.3 = 9.78\\\\\n"," G_5 &= 0\\\\\n"," G_4 &= R_5 + \\gamma G_5 = 2\\\\\n"," G_{0,\\dots 5} &= (0.915, 8.192, 9.78, 6.3, 2, 0)\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"x02-3W5DjuKB"},"source":["### 3. (1 point) Suppose $\\gamma= 0.8$ and the reward sequence is $R_1 = 3$ followed by an infinite sequence of $6$s. What are $G_1$ and $G_0$?"]},{"cell_type":"markdown","metadata":{"id":"9OnTjkuFkIuy"},"source":["$$\n","\\begin{align}\n","G_1 &= R_2 + \\gamma (R_3 + \\gamma R_4 + \\gamma^2R_5 + \\dots\\\\\n","&= 6 + 0.8(6+0.8\\times 6 + 0.8^2\\times 6 + \\dots)\\\\\n","G_1 &= \\sum_{k=0}^{\\infty} \\gamma^kR_{const} = \\frac{R_{const}}{1-\\gamma} = \\frac{6}{1 - 0.8} = 30\\\\\n","G_0 &= R_1 + \\gamma G_1 = 3 + 0.8\\times 30 = 27\\\\\n","G_{0,1} &= (27, 30)\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"1O4--A7Zmmsu"},"source":["### 4. (2 points) Exercise 3.11: If the current state is $S_t$, and actions are selected according to a stochastic policy $\\pi$, then what is the expectation of $R_{t+1}$ in terms of $\\pi$ and the four-argument function p(s', r | s, a) (3.2)?\n","Hint: Eq 3.5 is helpful. And also note that $\\pi$ defines P(a|s)."]},{"cell_type":"markdown","metadata":{"id":"5iIiKj5WnOAY"},"source":["Given that we know probability $ p(r|s,a) = \\sum_{s' \\in S}p(s',r|s,a)$, how do we find the expectation value for $R_t$? Since a is a conditional, we also need the probability for a.\n","\n","$$\n","\\begin{align}\n","\\mathbb{E}[R_t|S_{t-1}=s, A_{t-1}=a] &= \\sum_{r \\in R}r\\cdot p(r|s,a)\\\\\n","&= \\sum_{r\\in R}r\\sum_{s'\\in S}p(s',r|s,a)\\\\\n","\\mathbb{E}[R_t|S_{t-1}=s] &= \\sum_{a\\in A}\\pi(a|s)\\sum_{r\\in R}r\\sum_{s'\\in S}p(s',r|s,a)\\\\\n","&= \\mathbb{E}[R_{t+1}|S_t=s].\n","\\end{align}\n","$$"]},{"cell_type":"markdown","metadata":{"id":"pHS4X_p2ukYh"},"source":["### 5. (2 points) A variation of Exercise 3.14. The Bellman equation (3.14) must hold for each state for the value function $v_{\\pi}$ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the state A', valued at -1.3, with respect to its three neighboring states, valued at -1.9, -0.4, and -1.2. Hint: Don't forget to include what happens when the agent moves south."]},{"cell_type":"markdown","metadata":{"id":"ncY5Ga9JvSpK"},"source":[""]},{"cell_type":"markdown","metadata":{"id":"OWvRlBHQuvFm"},"source":["### 6. (2 points) The middle subfigure of Figure 3.5 shows the optimal state-value function, $v_{\\ast}$ of the gridworld. Use these values to calculate the 4 q-values associated with state A'. Explain why the value v(s=A') = 16.0 satisfies the Bellman optimality equation."]},{"cell_type":"markdown","metadata":{"id":"VCxA4A-RvSLs"},"source":["The Bellman Equation:\n","$$\n","v_\\pi(s) = \\mathbb{E}_\\pi[G_t|S_t=s] = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r+\\gamma \\mathbb{E}_\\pi[G_{t+1}|S_{t+1}=s']\\right]\n","$$"]},{"cell_type":"markdown","metadata":{"id":"7BjoIw6417Ln"},"source":[""]}]}