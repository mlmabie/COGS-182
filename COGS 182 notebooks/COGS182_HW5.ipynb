{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"COGS182_HW5.ipynb","provenance":[{"file_id":"1dIlmG6Zvfbj2RRGCh948xxTkMLN-yY6U","timestamp":1613030449566},{"file_id":"1EV3oAN1D5R3xwn7uCU_LlvorGdY1S9T6","timestamp":1612301835074},{"file_id":"1oLO2_HDUlJdj9xeVeWKgrLRVEwmqcG2a","timestamp":1612083522062}],"collapsed_sections":[],"authorship_tag":"ABX9TyPUwS+p399BDqmHBZjP0e1Y"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"1pRwde7UCX2A"},"source":["## COGS 182 Homework 5\n","Malachi Mabie \n","10 Feb 2021"]},{"cell_type":"markdown","metadata":{"id":"KPIZVydU8jtg"},"source":["###Q1 (2 points)\n","Exercise 5.4 The pseudocode for Monte Carlo ES is inefficient because, for each state-action pair, it maintains a list of all returns and repeatedly calculates their mean. It would be more efficient to use techniques similar to those explained in Section 2.4 to maintain just the mean and a count (for each state-action pair) and update them incrementally. Describe how the pseudocode would be altered to achieve this."]},{"cell_type":"markdown","metadata":{"id":"5eNHwH4Sf7qg"},"source":["To update a mean without retaining list of historical data, you would initialize count and update avg per step as:\n","```\n","avg = (avg*count + return)/(++count)\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xrSu5hPevFTS"},"source":["In exercise 5.4, the relevant pseudocode is:\n","\n","\n","```\n","  Append G to Returns(S_t, A_t)\n","  Q(S_t, A_t) <- average(Returns(S_t, A_t))\n","```\n","Instead, we can do:\n","\n","```\n","  N[S,A] += 1 // update combination frequency\n","  Q[S, A] += G - Q[S,A] / N[S,A]\n","```\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"M2-FpC1mwL1l"},"source":["Instead of a large list of returns, we add a variable that counts the number of states and actions: ```1/N(new experience - old avg).``` \n","\n"]},{"cell_type":"markdown","metadata":{"id":"oLXMwTfs84km"},"source":["###Q2 (4 points)\n","Use the Monte Carlo Exploring Starts algorithm from page 99 to generate a policy and value functions for the gridworld of example 4.1. Display the value function and policy after the 10, 100, 300, and 1000 episodes. Does the algorithm converge to an optimal policy? [You don't need to worry about making the policy display pretty. You can use matrices to show the probability of each action.]"]},{"cell_type":"code","metadata":{"id":"yYy-Lssl87QC","executionInfo":{"status":"ok","timestamp":1614219704883,"user_tz":600,"elapsed":484,"user":{"displayName":"Malachi Mabie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBeiIGEiVq8-CE_kpklRz2oFWae97kCAfHGVaW=s64","userId":"10152145272942493416"}}},"source":["import numpy as np\n","\n","transitions = np.array([[0,0,0,0],[0,1,2,5],[1,2,3,6],[2,3,3,7],\n","                        [4,0,5,8],[4,1,6,9],[5,2,7,10],[6,3,7,11],\n","                        [8,4,9,12],[8,5,10,13],[9,6,11,14],[10,7,11,0],\n","                        [12,8,13,12],[12,9,14,13],[13,10,0,14],[0,0,0,0]])"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MgAukVDl8dBz","executionInfo":{"status":"ok","timestamp":1614219707538,"user_tz":600,"elapsed":918,"user":{"displayName":"Malachi Mabie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBeiIGEiVq8-CE_kpklRz2oFWae97kCAfHGVaW=s64","userId":"10152145272942493416"}},"outputId":"222479f4-7df4-4106-e02e-9f1d72098210"},"source":["num_states = transitions.shape[0]\n","values = np.zeros(num_states)\n","print_at = [10,100,300,1000,10000]\n","\n","# exploring starts means initialize policy randomly\n","policy = np.random.randint(low=0,high=3,size=num_states)\n","\n","# q vals and state action counts start at 0\n","qvalues = np.zeros([num_states, 4])\n","sa_count = np.zeros([num_states, 4])\n","\n","for i in range(1,10001):\n","  state = np.random.randint(0,15) # allows it to find endzone initially\n","  action = policy[state]\n","\n","  ep_sa = []\n","  ep_reward = []\n","\n","  # generate an episode - limit loops to prevent infinite\n","  for step in range(12):\n","    # encode state-action pair\n","    ep_sa.append(10*state + action) # unique bc actions are single digit\n","\n","    if state == 0:\n","      ep_reward.append(0) # endzone\n","      break\n","    ep_reward.append(-1) # constant neg reward\n","\n","    next_state = transitions[state][action] # get next\n","\n","    # if next_state == 0:\n","    #   break # for some reason when i don't add this, some edge values get stuck\n","\n","    state = next_state\n","    action = policy[state]\n","\n","  G = 0\n","  # loop backwards through episode\n","  for i_ep, sa in enumerate(reversed(ep_sa)):\n","    # unencode indice of state-action\n","    sa_indice = len(ep_reward) - i_ep - 1\n","    G += ep_reward[sa_indice]\n","    # unencode state and action from sa\n","    state = sa//10\n","    action = sa%10\n","\n","    # did sa occur earlier in ep?\n","    if sa not in ep_sa[:sa_indice]:\n","      sa_count[state][action] += 1\n","      # perform the avg rewards trick:\n","      qvalues[state][action] += (G - qvalues[state][action]) / sa_count[state][action]\n","      # update policy by best action that could have been taken within that state\n","      policy[state] = np.argmax(qvalues[state])\n","\n","  # print requested iterations\n","  if i in print_at:\n","    print('k = ',i)\n","    print(np.round(np.max(qvalues,axis=1),1).reshape(4,4))\n","    "],"execution_count":2,"outputs":[{"output_type":"stream","text":["k =  10\n","[[  0.   0.   0. -12.]\n"," [  0.   0.   0.   0.]\n"," [  0.   0.   0.   0.]\n"," [  0.   0.   0.   0.]]\n","k =  100\n","[[  0.   -1.   -5.4  -8.7]\n"," [ -1.   -2.3  -3.5  -4.7]\n"," [ -2.4  -3.   -6.   -1. ]\n"," [-11.7  -5.1  -1.    0. ]]\n","k =  300\n","[[  0.   -1.   -4.4  -6.6]\n"," [ -1.   -2.1  -3.2  -4.2]\n"," [ -2.1  -3.   -5.   -1. ]\n"," [-11.9  -4.5  -1.    0. ]]\n","k =  1000\n","[[  0.   -1.   -4.1  -5.6]\n"," [ -1.   -2.   -3.   -4.1]\n"," [ -2.   -3.   -4.2  -1. ]\n"," [-12.   -4.2  -1.    0. ]]\n","k =  10000\n","[[  0.   -1.   -4.   -5.1]\n"," [ -1.   -2.   -3.   -4. ]\n"," [ -2.   -3.   -4.   -1. ]\n"," [-12.   -4.   -1.    0. ]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"kaAFD-qNPk9K"},"source":["(Notice how it relies on biased memories, so some equal paths are weighted with preference for one over the other. However, the latest run didn't give anything severe outlying penalties, so it converged to an equanimous solution.)"]},{"cell_type":"markdown","metadata":{"id":"SI82lNhA87aU"},"source":["###Q3 (2 points)\n","Suppose that the agent was forced to always start in state 6. What changes would you need to make in order to still have the same convergence properties? [You do not need to code the change.]"]},{"cell_type":"markdown","metadata":{"id":"4wYx7JXQTdDU"},"source":["\n","\n","> If it always starts from state 6, the current max policy would be effected by the extra state 6 experiences. Changing it to epsilon-greedy would solve this problem.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Pa_MFtQ989zx"},"source":["###Q4 (2 points)\n","Suppose that you are using an off-policy Monte Carlo algorithm with importance sampling. You observe two state-action trajectories both starting from state, $s_0$. Both trajectories had the same probability under the target policy $\\pi$. However, the first trajectory had a low probability of occurring under the behavior policy, $b$ and resulted in a $G_t=10$. The second trajectory had a high probability of occurring under the behavior policy, $b$ and resulted in a $G_t=0$. What can we say about the estimate, $v_\\pi(s_0)$? Why?"]},{"cell_type":"markdown","metadata":{"id":"x-iuq_DzEsd6"},"source":["\n","\n","> We want to get $\\mathbb{E}[\\rho_{t:0} G_t | S_t=s]=v_{\\pi}(s)$. The importance ratio is determined by comparing the likelihood of the first trajectory in the behavior policy and the target policy. Probability in pi is always 0.5, and probability in b for step 0 is \"low\". 0.5/\"low\" is going to be a large weight, while the second step has 0.5/\"high\" and therefore a lower weight. So, the expected value of step 0 will be very similar to the behavior policy of trajectory 1.\n","\n"]},{"cell_type":"markdown","metadata":{"id":"QGd0nAH6Ua21"},"source":["$$\n","\\rho_{t:T-1}\\dot{=}\\frac{\\prod_{k=t}^{T-1}\\pi(A_k|S_k)p(S_{k+t}|S_k,A_k}{\\prod_{k=t}^{T-1}b(A_k|S_k)p(S_{k+t}|S_k,A_k} = \\prod_{k=t}^{T-1}\\frac{\\pi(A_k|S_k)}{b(A_k|S_k)}\\\\\\\\\n","V(s) \\dot{=}\\frac{\\sum_{t\\in\\tau(s)}\\rho_{t:T(t)-1}G_t}{\\sum_{t\\in\\tau(s)}\\rho_{t:T(t)-1}} = \\rho(10)+\\rho(0)\n","$$"]}]}