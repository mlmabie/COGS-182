{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"HW7_skeleton.ipynb","provenance":[{"file_id":"1P4E4Bz0v2Z3cME5Id6He4f1svGzg_mdV","timestamp":1614389665606}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"0xc-k4opGCMj"},"source":["##I will be walking through both n-step TD and TD($\\lambda$) in the discussion on Friday and will offer a bit of skeleton code, especially since the indexing in n-step TD can be tricky. "]},{"cell_type":"code","metadata":{"id":"z2HhJfJLAvEP"},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wzKEmzNWMFVD"},"source":["look at returns over multiple steps to update your Q. But you don't know the policy for the future so you look back."]},{"cell_type":"code","metadata":{"id":"_H7k52jkA0yF"},"source":["#Create a step function for the Random Walk in Example 6.2\n","#state = 0 is the terminal state\n","def create_step(num_states):\n","\n","    def step(state):\n","\n","        if np.random.random() > 0.5:\n","            state_new = state + 1\n","        else:\n","            state_new = state - 1\n","\n","        r = 0\n","        if state_new == num_states:\n","            r = 1\n","            state_new = 0\n","\n","        return state_new, r\n","    \n","    return step"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zlfDNkBVA2v-","colab":{"base_uri":"https://localhost:8080/","height":130},"executionInfo":{"status":"error","timestamp":1614389087880,"user_tz":600,"elapsed":849,"user":{"displayName":"Malachi Mabie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBeiIGEiVq8-CE_kpklRz2oFWae97kCAfHGVaW=s64","userId":"10152145272942493416"}},"outputId":"edbb05bd-39ff-4b16-fa5b-05a3f6899fad"},"source":["def run_episode_TD_lmbda(step, start_state, values, lmdba, alpha, gamma):\n","    \n","    #initialize weights and starting state\n","    e_weights = np.zeros(len(values))\n","    state = start_state\n","   \n","    for t in range(10000):\n","        \n","        state_new, r      = \n","        td_error          = \n","        e_weights[state]  = 1 #replacing traces\n","        \n","        #update values and eligibility weights\n","        values            = \n","        e_weights         = \n","        \n","        state             = state_new \n","        \n","        if state == 0:\n","            break\n","        \n","    return values"],"execution_count":null,"outputs":[{"output_type":"error","ename":"SyntaxError","evalue":"ignored","traceback":["\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-99a98cef384c>\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    state_new, r      =\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"]}]},{"cell_type":"code","metadata":{"id":"Olk8YmhmA7Lb"},"source":["def run_episode_nstep_TD(step, start_state, values, n, alpha, gamma):\n","    \n","    #Keep track of state and reward at each t\n","    state_trajectory  = []\n","    reward_trajectory = []\n","                \n","    state = start_state\n","    state_trajectory.append(state)\n","    reward_trajectory.append(0)\n","    \n","    #Step through episode\n","    for t in range(10000):\n","\n","        #Take one step\n","        state_new, r = step(state)\n","        state_trajectory.append(state_new)\n","        reward_trajectory.append(r)\n","        state = state_new\n","\n","        #n step backup if not in terminal state\n","        if t >= n-1 and state !=0:\n","\n","            state_update = state_trajectory[t-n+1]\n","\n","            G = 0\n","            #iterate back over the nsteps of the back up adding reward\n","            for tau in range(0, n):\n","                    G += \n","            \n","            #add discounted value of the final state\n","            G += \n","            \n","            #update desired state value\n","            values[state_update] = \n","\n","\n","        #if in terminal state complete all back\n","        if state == 0:\n","\n","            G = 0  \n","            for i in range(n):\n","                \n","                # G as a function of the relavent reward and discounted old G\n","                G = \n","\n","                state_update = state_trajectory[t-i]\n","                #update desire state\n","                values[state_update] = \n","\n","                if t-i == 0:\n","                    break\n","\n","            break\n","            \n","    return values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxEzYUewA7ey","colab":{"base_uri":"https://localhost:8080/","height":266},"executionInfo":{"status":"error","timestamp":1614390959168,"user_tz":600,"elapsed":711,"user":{"displayName":"Malachi Mabie","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhBeiIGEiVq8-CE_kpklRz2oFWae97kCAfHGVaW=s64","userId":"10152145272942493416"}},"outputId":"48cb9cae-4944-4cef-808f-c3cbcb7220d2"},"source":["# keep track of the RMS error\n","num_states = 19+1 \n","start_state = np.int(num_states/2)\n","true_values = np.linspace(0,1,num_states+1)[:-1]\n","step = create_step(num_states)\n","\n","\n","alpha  = 0.1\n","ns     = [1,4,16,64] # possible values for n\n","lmbdas = [0,0.5,0.9,1]\n","gamma  = 1.0\n","\n","\n","num_runs      = 100\n","num_episodes  = 100\n","\n","#Store the RMS for each run\n","td_RMS        = np.zeros([len(ns), num_runs, num_episodes+1])\n","lmbda_RMS     = np.zeros([len(ns), num_runs, num_episodes+1])\n","# the core is capturing this value ^\n","\n","    \n","#n-step TD\n","for i, n in enumerate(ns):\n","    for j in range(num_runs):\n","    \n","        #Initialize values\n","        values          = np.ones(num_states)*0 \n","        values[0]       = 0          \n","        td_RMS[i, j, 0] = np.sqrt(np.sum((values-true_values)**2)/len(values))\n","\n","        #Loop through episodes while updating values\n","        for k in range(num_episodes):\n","\n","            values            = run_episode_nstep_TD(step, start_state, values, n, alpha, gamma) \n","            td_RMS[i, j, k+1] = np.sqrt(np.sum((values-true_values)**2)/len(values)) \n","            \n","\n","            \n","#TD lambda            \n","for i, lmbda in enumerate(lmbdas):\n","    for j in range(num_runs):\n","\n","        #Initialize values\n","        values = np.ones(num_states)*0 \n","        values[0] = 0\n","        lmbda_RMS[i, j, 0] = np.sqrt(np.sum((values-true_values)**2)/len(values))\n","\n","        #Loop through episodes while updating values\n","        for k in range(num_episodes):\n","\n","            values = run_episode_TD_lmbda(step, start_state, values, lmbda, alpha, gamma)\n","            lmbda_RMS[i, j, k+1] = np.sqrt(np.sum((values-true_values)**2)/len(values)) "],"execution_count":null,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-2f600e9f9f9e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m             \u001b[0mvalues\u001b[0m            \u001b[0;34m=\u001b[0m \u001b[0mrun_episode_nstep_TD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m             \u001b[0mtd_RMS\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrue_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'run_episode_nstep_TD' is not defined"]}]},{"cell_type":"code","metadata":{"id":"_vM1v2t1A7sS"},"source":["fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n","\n","for i in range(4):\n","    \n","    ax[0].plot(np.mean(td_RMS[i,:,:], axis=(0)).T, label = \"N=\" + str(ns[i]) )\n","    ax[1].plot(np.mean(lmbda_RMS[i,:,:], axis=(0)).T, label = \"l=\" + str(lmbdas[i]))\n","    \n","for i in range(2):\n","    ax[i].legend()\n","    ax[i].set_ylim(ymin=0, ymax=0.6)\n","    ax[i].set_xlabel(\"Walks/Episodes\")\n","\n","ax[0].title.set_text(\"n-step TD\")\n","ax[1].title.set_text(\"TD(lambda)\")\n","ax[0].set_ylabel(\"Empirical RMS error\")"],"execution_count":null,"outputs":[]}]}