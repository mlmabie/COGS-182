{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Copy of HW8_skeleton.ipynb","provenance":[{"file_id":"1P4E4Bz0v2Z3cME5Id6He4f1svGzg_mdV","timestamp":1614378859873}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"z2HhJfJLAvEP"},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_H7k52jkA0yF"},"source":["#Create a step function for the Random Walk in Example 6.2\n","#state = 0 is the terminal state\n","def create_step(num_states):\n","\n","    def step(state):\n","\n","        if np.random.random() > 0.5:\n","            state_new = state + 1\n","        else:\n","            state_new = state - 1\n","\n","        r = 0\n","        if state_new == num_states:\n","            r = 1\n","            state_new = 0\n","\n","        return state_new, r\n","    \n","    return step"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zlfDNkBVA2v-"},"source":["def run_episode_TD_lmbda(step, start_state, values, lmdba, alpha, gamma):\n","    \n","    #initialize weights and starting state\n","    e_weights = np.zeros(len(values))\n","    state = start_state\n","   \n","    for t in range(10000):\n","        \n","        state_new, r      = \n","        td_error          = \n","        e_weights[state]  = 1 #replacing traces\n","        \n","        #update values and eligibility weights\n","        values            = \n","        e_weights         = \n","        \n","        state             = state_new \n","        \n","        if state == 0:\n","            break\n","        \n","    return values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Olk8YmhmA7Lb"},"source":["def run_episode_nstep_TD(step, start_state, values, n, alpha, gamma):\n","    \n","    #Keep track of state and reward at each t\n","    state_trajectory  = []\n","    reward_trajectory = []\n","                \n","    state = start_state\n","    state_trajectory.append(state)\n","    reward_trajectory.append(0)\n","    \n","    #Step through episode\n","    for t in range(10000):\n","\n","        #Take one step\n","        state_new, r = step(state)\n","        state_trajectory.append(state_new)\n","        reward_trajectory.append(r)\n","        state = state_new\n","\n","        #n step backup if not in terminal state\n","        if t >= n-1 and state !=0:\n","\n","            state_update = state_trajectory[t-n+1]\n","\n","            G = 0\n","            #iterate back over the nsteps of the back up adding reward\n","            for tau in range(0, n):\n","                    G += \n","            \n","            #add discounted value of the final state\n","            G += \n","            \n","            #update desired state value\n","            values[state_update] = \n","\n","\n","        #if in terminal state complete all back\n","        if state == 0:\n","\n","            G = 0  \n","            for i in range(n):\n","                \n","                # G as a function of the relavent reward and discounted old G\n","                G = \n","\n","                state_update = state_trajectory[t-i]\n","                #update desire state\n","                values[state_update] = \n","\n","                if t-i == 0:\n","                    break\n","\n","            break\n","            \n","    return values"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TxEzYUewA7ey"},"source":["num_states = 19+1 \n","start_state = np.int(num_states/2)\n","true_values = np.linspace(0,1,num_states+1)[:-1]\n","step = create_step(num_states)\n","\n","\n","alpha  = 0.1\n","ns     = [1,4,16,64]\n","lmbdas = [0,0.5,0.9,1]\n","gamma  = 1.0\n","\n","\n","num_runs      = 100\n","num_episodes  = 100\n","\n","#Store the RMS for each run\n","td_RMS        = np.zeros([len(ns), num_runs, num_episodes+1])\n","lmbda_RMS     = np.zeros([len(ns), num_runs, num_episodes+1])\n","\n","\n","    \n","#n-step TD\n","for i, n in enumerate(ns):\n","    for j in range(num_runs):\n","    \n","        #Initialize values\n","        values          = np.ones(num_states)*0 \n","        values[0]       = 0          \n","        td_RMS[i, j, 0] = np.sqrt(np.sum((values-true_values)**2)/len(values))\n","\n","        #Loop through episodes while updating values\n","        for k in range(num_episodes):\n","\n","            values            = run_episode_nstep_TD(step, start_state, values, n, alpha, gamma) \n","            td_RMS[i, j, k+1] = np.sqrt(np.sum((values-true_values)**2)/len(values)) \n","            \n","\n","            \n","#TD lambda            \n","for i, lmbda in enumerate(lmbdas):\n","    for j in range(num_runs):\n","\n","        #Initialize values\n","        values = np.ones(num_states)*0 \n","        values[0] = 0\n","        lmbda_RMS[i, j, 0] = np.sqrt(np.sum((values-true_values)**2)/len(values))\n","\n","        #Loop through episodes while updating values\n","        for k in range(num_episodes):\n","\n","            values = run_episode_TD_lmbda(step, start_state, values, lmbda, alpha, gamma)\n","            lmbda_RMS[i, j, k+1] = np.sqrt(np.sum((values-true_values)**2)/len(values)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_vM1v2t1A7sS"},"source":["fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(13,5))\n","\n","for i in range(4):\n","    \n","    ax[0].plot(np.mean(td_RMS[i,:,:], axis=(0)).T, label = \"N=\" + str(ns[i]) )\n","    ax[1].plot(np.mean(lmbda_RMS[i,:,:], axis=(0)).T, label = \"l=\" + str(lmbdas[i]))\n","    \n","for i in range(2):\n","    ax[i].legend()\n","    ax[i].set_ylim(ymin=0, ymax=0.6)\n","    ax[i].set_xlabel(\"Walks/Episodes\")\n","\n","ax[0].title.set_text(\"n-step TD\")\n","ax[1].title.set_text(\"TD(lambda)\")\n","ax[0].set_ylabel(\"Empirical RMS error\")"],"execution_count":null,"outputs":[]}]}